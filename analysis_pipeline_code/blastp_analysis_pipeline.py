import re
import sys
import yaml
import traceback
import subprocess
import pandas as pd
import xml.etree.ElementTree as ET
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed

from MME_Web_Tool_CLI.analysis_pipeline_code.shared_analysis_steps import (
    IEDB_analyze_matches,
    calc_reference_table,
    calc_epitope_table,
    calc_query_table,
)

#åˆ‡ç‰‡åœ¨DefåŠ ä¸Šåº§æ¨™ï¼Œä¿ç•™åºåˆ—åŸå§‹é•·åº¦
def sliding_windows_fasta(fasta_text, window_size, group_size=100):
    """
    å°å¤šæ¢ FASTA åºåˆ—é€²è¡Œ sliding window åˆ‡ç‰‡ï¼Œæ¯ group_size æ¢åˆ†çµ„ã€‚

    å›å‚³:
        fasta_groups: List[str]ï¼Œåˆ†çµ„å¥½çš„ FASTA åºåˆ—
        original_lengths: Dict[str, int]ï¼Œè¨˜éŒ„æ¯æ¢åŸå§‹åºåˆ—é•·åº¦
    """
    fasta_groups = []
    current_group = []
    original_lengths = {}

    lines = fasta_text.strip().splitlines()
    title = None
    sequence = ""

    def process_sequence(title, sequence):
        result = []
        if not title or not sequence:
            return result
        original_lengths[title] = len(sequence)
        if len(sequence) < window_size or window_size == 0:
            result.append(f">{title}\n{sequence}")
            return result
        for i in range(len(sequence) - window_size + 1):
            start = i + 1
            end = i + window_size
            header = f">{title}_{start}_{end}"
            fragment = sequence[i:end]
            result.append(f"{header}\n{fragment}")
        return result

    for line in lines:
        line = line.strip()
        if line.startswith(">"):
            if title is not None:
                fragments = process_sequence(title, sequence)
                for frag in fragments:
                    current_group.append(frag)
                    if len(current_group) == group_size:
                        fasta_groups.append("\n".join(current_group))
                        current_group = []
            title = line[1:].strip()
            sequence = ""
        else:
            sequence += line

    # è™•ç†æœ€å¾Œä¸€æ¢åºåˆ—
    if title:
        fragments = process_sequence(title, sequence)
        for frag in fragments:
            current_group.append(frag)
            if len(current_group) == group_size:
                fasta_groups.append("\n".join(current_group))
                current_group = []

    if current_group:
        fasta_groups.append("\n".join(current_group))

    return fasta_groups, original_lengths
#run blastp
def run_blastp(fasta,db,evalue,matrix,gap):
    if gap == '0' :

        result = subprocess.run(
            [
                "blastp", 'blastp-short',
                "-query", "-",
                "-db", db,
                "-outfmt", "5",
                "-evalue", str(evalue),
                "-matrix", matrix,
                "-ungapped",
                "-comp_based_stats", "0",
                "-word_size", "2",
                "-out", "-"  # è¼¸å‡ºåˆ° stdout
            ],
            input=fasta.encode(),  # ç›´æ¥ä»¥ FASTA å­—ç¬¦ä¸²ä½œç‚ºè¼¸å…¥
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
    else:


        result = subprocess.run(
            [
                "blastp",
                "-db", db,
                "-outfmt", "5",
                "-evalue", str(evalue),
                "-matrix", matrix,

                "-word_size", "2",
                "-out", "-"  # è¼¸å‡ºåˆ° stdout
            ],
            input=fasta.encode(),  # ç›´æ¥ä»¥ FASTA å­—ç¬¦ä¸²ä½œç‚ºè¼¸å…¥
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
    blast_text = result.stdout.decode('utf-8')
    df = parse_blast_xml_to_dataframe(blast_text)
    return df
#xlm to csv
def parse_blast_xml_to_dataframe(xml_text):
    """å°‡ BLAST XML çµæœè½‰ç‚º pandas DataFrame"""
    root = ET.fromstring(xml_text)
    records = []

    for iteration in root.find('BlastOutput_iterations').findall('Iteration'):
        query_id = iteration.findtext("Iteration_query-ID")
        query_def = iteration.findtext("Iteration_query-def")
        query_len = iteration.findtext("Iteration_query-len")

        hits = iteration.find("Iteration_hits")
        if hits is not None and hits.find("Hit") is not None:
            for hit in hits.findall("Hit"):
                hit_num = hit.findtext("Hit_num")
                hit_def = hit.findtext("Hit_def")
                hit_len = hit.findtext("Hit_len")

                hsps = hit.find("Hit_hsps")
                if hsps is not None:
                    for hsp in hsps.findall("Hsp"):
                        bit_score = hsp.findtext("Hsp_bit-score")
                        score = hsp.findtext("Hsp_score")
                        evalue = hsp.findtext("Hsp_evalue")
                        identity = hsp.findtext("Hsp_identity")
                        positive = hsp.findtext("Hsp_positive")
                        gaps = hsp.findtext("Hsp_gaps")
                        align_len = hsp.findtext("Hsp_align-len")

                        qseq = hsp.findtext("Hsp_qseq")
                        hseq = hsp.findtext("Hsp_hseq")
                        midline = hsp.findtext("Hsp_midline")
                        query_from = hsp.findtext("Hsp_query-from")
                        query_to = hsp.findtext("Hsp_query-to")
                        hit_from = hsp.findtext("Hsp_hit-from")
                        hit_to = hsp.findtext("Hsp_hit-to")
                        identity_percent = round(int(identity) / int(align_len) * 100, 1)
                        positive_percent = round(int(positive) / int(align_len) * 100, 1)
                        gaps_percent = round(int(gaps) / int(align_len) * 100, 1)
                        Query_MME_Len = int(query_to) - int(query_from) + 1
                        Hit_MME_Len=int(hit_to) - int(hit_from) + 1
                        records.append([
                            query_id, query_def, query_len,
                            hit_num, hit_def, hit_len,
                            bit_score, score, evalue,
                            identity, positive, gaps,identity_percent, positive_percent, gaps_percent, Query_MME_Len,
                            query_from, query_to, align_len,Hit_MME_Len, hit_from, hit_to,
                            qseq, hseq, midline
                        ])

    columns = [
        "Query_ID","Query_Def", "Query_Len",
        "Hit_Num", "Hit_Def", "Hit_Len",
        "Bit_Score", "Score", "Evalue",
        "Identity", "Positive", "Gaps","Identity%", "Positive%", "Gaps%", "Query_MME_Len",
        "Query_From", "Query_To", "Align_Len","Hit_MME_Len","Hit_From", "Hit_To",
        "Query_Seq", "Hit_Seq", "Midline"
    ]
    return pd.DataFrame(records, columns=columns)
##å¤šå·¥blastp
def process_one_group(fasta_chunk, blast_db, evalue, matrix, gap):
        try:
            blast_results = run_blastp(fasta_chunk, blast_db, evalue, matrix, gap)

            return blast_results
        except Exception as e:
            return f"[ERROR] {e}"
def run_all_groups_multicore(fasta_groups, blast_db, evalue, matrix, gap):
        all_results = []

        with ProcessPoolExecutor() as executor:
            futures = [
                executor.submit(process_one_group, chunk, blast_db, evalue, matrix, gap)
                for chunk in fasta_groups
            ]

            for i, future in enumerate(as_completed(futures), 1):
                try:
                    result = future.result()
                    print(f"âœ… Group {i} finished.")
                    # print(result)
                    all_results.append(result)
                except Exception as e:
                    print(f"âŒ Error in group {i}: {e}")
                    all_results.append(None)

            # éæ¿¾æ‰ None æˆ–éŒ¯èª¤è¨Šæ¯ï¼Œåªä¿ç•™ DataFrame
            valid_results = [df for df in all_results if isinstance(df, pd.DataFrame)]

            # åˆä½µæˆä¸€å¼µç¸½è¡¨
            final_df = pd.concat(valid_results, ignore_index=True)
            # âœ… å¦‚æœçµæœç‚ºç©ºå°±ç›´æ¥å›å‚³
            if final_df.empty:
                print("âš ï¸ No results found, returning empty DataFrame.")
                return final_df
        return final_df
####queryåº§æ¨™è™•ç†####
def adjust_query_coordinates(df, kmer):
    if kmer == 0:
        # ä¸åšä¿®æ­£
        return df

    def extract_base(def_str):
        match = re.search(r"_(\d+)_(\d+)$", def_str)
        return int(match.group(1)) if match else 0

    def compute_start(row):
        base = extract_base(row["Query_Def"])
        return base + int(row["Query_From"]) - 1

    def compute_end(row):
        base = extract_base(row["Query_Def"])
        return base + int(row["Query_To"]) - 1

    df["Query_From"] = df.apply(compute_start, axis=1)
    df["Query_To"] = df.apply(compute_end, axis=1)

    return df
#æ ¹æ“š Query_Def è¨ˆç®—åŸå§‹é•·åº¦ä¸¦å–ä»£ Query_Len æ¬„ä½
def extract_original_def(def_str):
    match = re.match(r"(.+?)_(\d+)_(\d+)$", def_str)
    return match.group(1) if match else def_str
#æŠŠ Query_Def ä¸Šçš„åº§æ¨™å»æ‰
def strip_suffix_if_kmer(query_def, kmer):
        if int(kmer) > 0:
            # åŒ¹é… _æ•¸å­—_æ•¸å­— çµå°¾
            return re.sub(r'_(\d+)_(\d+)$', '', query_def)
        return query_def
#å»é‡è¤‡ä¸¦æŠŠEvalueæœ€å°çš„ç•™ä¸‹
def dedup_stay_small_evalue(data):
    data["Evalue"] = pd.to_numeric(data["Evalue"], errors="coerce")  # ç¢ºä¿æ˜¯æ•¸å€¼
    final_df = (
        data.sort_values("Evalue")  # å…ˆæ’åºï¼ŒEvalueå°çš„æœƒæ’å‰é¢
            .groupby(["Query_Def","Query_From" ,"Query_To","Hit_Def", "Hit_From", "Hit_To", "Hit_Seq"], as_index=False)
            .first()  # æ¯çµ„åªä¿ç•™ Evalue æœ€å°çš„é‚£ç­†
        )
    return final_df
#äºŒæ¬¡ç¯©é¸(é•·åº¦ç¯„åœã€identityã€positiveã€gap)
def second_filter(blastp_data,min_length,max_length,identity,positive,gap_num):
            # å°‡ identity å¾ç™¾åˆ†æ¯”è½‰æˆå°æ•¸
            identity_threshold = identity * 0.01
            positive_threshold = positive * 0.01
            gap_threshold = gap_num * 0.01
            # å…ˆæŠŠ Align_Len è½‰æˆæ•´æ•¸
            blastp_data['Align_Len'] = blastp_data['Align_Len'].astype(int)

            # å¦‚æœ Identity ä¹Ÿæ˜¯å­—ä¸²ï¼Œä¹Ÿä¸€ä½µè½‰æˆæ•¸å­—
            blastp_data['Identity'] = blastp_data['Identity'].astype(float)
            blastp_data['Positive'] = blastp_data['Positive'].astype(float)
            blastp_data['Gaps'] = blastp_data['Gaps'].astype(float)
            # å»ºç«‹ maskï¼Œé è¨­å…¨ True
            mask = pd.Series(True, index=blastp_data.index)
            # ç¯©é¸æœ€å°é•·åº¦
            if min_length > 0:
                mask &= blastp_data['Align_Len'] >= min_length

            # ç¯©é¸æœ€å¤§é•·åº¦
            if max_length > 0:
                mask &= blastp_data['Align_Len'] <= max_length

            # ç¯©é¸ Identity/Align_Len å¤§æ–¼é–¾å€¼
            if identity > 0:
                mask &= ((blastp_data['Identity'] / blastp_data['Align_Len']).round(2)) >= identity_threshold
            # ç¯©é¸ positive/Align_Len å¤§æ–¼é–¾å€¼
            if positive > 0:
                mask &= ((blastp_data['Positive'] / blastp_data['Align_Len']).round(2)) >= positive_threshold
            # ç¯©é¸ Identity/Align_Len å¤§æ–¼é–¾å€¼
            if gap_num > 0:
                mask &= ((blastp_data['Gaps'] / blastp_data['Align_Len']).round(2)) <= gap_threshold

            # å¥—ç”¨ç¯©é¸
            blastp_data = blastp_data[mask]

            return blastp_data


#############################################################
#                     ä¸» Pipeline
#############################################################
def blastp_pipeline_main(job_id):
    global CURRENT_JOB_DIR

    # === å°ˆæ¡ˆ jobs æ ¹ç›®éŒ„ ===
    project_root = Path(__file__).resolve().parent.parent
    out_dir = project_root / "jobs" / job_id
    job_dir = out_dir
    CURRENT_JOB_DIR = out_dir

    # === Step 0: è¨­å®š log ===
    log_path = job_dir / "pipeline.log"

    class Tee:
        def __init__(self, *files):
            self.files = files
        def write(self, data):
            for f in self.files:
                f.write(data)
                f.flush()
        def flush(self):
            for f in self.files:
                f.flush()

    log_file = open(log_path, "w", encoding="utf-8")
    sys.stdout = Tee(sys.stdout, log_file)
    sys.stderr = Tee(sys.stderr, log_file)

    success = False

    try:
        print("ğŸ“ Log æª”æ¡ˆå»ºç«‹æˆåŠŸï¼š", log_path)
        print("ğŸš€ Pipeline é–‹å§‹åŸ·è¡Œ...\n")

        # === è®€å–è¨­å®šæª” ===
        config_path = job_dir / "config.yaml"
        if not config_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¨­å®šæª”: {config_path}")

        with open(config_path, "r") as f:
            config = yaml.safe_load(f)

        logic = config.get("Matching_Logic")
        if logic == "Blastp":
            blast_db = Path(config["blastp_paths"]["human_db_path"]).resolve()  
            query_fasta_path = Path(config["blastp_paths"]["query_fasta_path"]).resolve()
            sequence_text = query_fasta_path.read_text(encoding="utf-8")
            human_location_path = Path(config["blastp_paths"]["human_protein_detail_path"]).resolve()
            IEDB_path = Path(config["blastp_paths"]["IEDB_file_path"]).resolve()

            evalue = config["blastp_parameters"]["evalue"]
            matrix = config["blastp_parameters"]["matrix"]
            gap = config["blastp_parameters"]["gap"]
            kmer = config["blastp_parameters"]["kmer_blastp"]
            min_length = config["blastp_parameters"]["min_length"]
            max_length = config["blastp_parameters"]["max_length"]
            identity = config["blastp_parameters"]["identity"]
            positive = config["blastp_parameters"]["positive"]
            gap_num = config["blastp_parameters"]["gap_num"]

        print("Step 1ï¼šåˆ‡å‰² FASTA åºåˆ—ä¸­ ...")
        fasta_groups, original_lengths = sliding_windows_fasta(sequence_text, kmer, group_size=100)
        print(f"Step 1 å®Œæˆï¼šç”¢ç”Ÿ {len(fasta_groups)} çµ„ FASTA åˆ‡ç‰‡")

        print("Step 2ï¼šåŸ·è¡Œå¤šæ ¸å¿ƒ BLASTP ...")
        blastp_df = run_all_groups_multicore(fasta_groups, blast_db, evalue, matrix, gap)
        print("Step 2 å®Œæˆ")

        print("Step 3ï¼šèª¿æ•´åº§æ¨™èˆ‡ Query_Len ...")
        blastp_df_coordinate = adjust_query_coordinates(blastp_df, kmer)
        blastp_df_coordinate["Query_Len"] = blastp_df_coordinate["Query_Def"].apply(
            lambda qdef: original_lengths.get(extract_original_def(qdef), None)
        )
        blastp_df_coordinate["Query_Def"] = blastp_df_coordinate["Query_Def"].apply(lambda x: strip_suffix_if_kmer(x, kmer))
        print("Step 3 å®Œæˆ")

        print("Step 4ï¼šå»é‡è¤‡ä¸¦ä¿ç•™æœ€å° Evalue ...")
        blastp_df_coordinate_dedup = dedup_stay_small_evalue(blastp_df_coordinate)
        print("Step 4 å®Œæˆ")

        # é™³æ˜±ä¸ä¿®æ”¹éƒ¨åˆ†
        filtered_df = second_filter(
            blastp_df_coordinate_dedup,
            min_length, max_length, identity, positive, gap_num
        )
        
        filtered_df = filtered_df.rename(columns={
            "Query_Def": "query_protein_name",
            "Query_From": "MME(query)_start",
            "Query_To": "MME(query)_end",
            "Hit_Def": "hit_human_protein_name",
            "Hit_From": "MME(hit)_start",
            "Hit_To": "MME(hit)_end",
            "Hit_Seq": "MME(hit)",
            "Query_Len": "query_protein_length",
            "Hit_Len": "hit_human_protein_length",
            "Query_MME_Len": "length_of_MME(query)",
            "Hit_MME_Len": "length_of_MME(hit)",
            "Query_Seq": "MME(query)",
        })
        filtered_df.drop(columns=["Query_ID","Hit_Num"], inplace=True)

        core_columns = [
            "MME(query)",
            "MME(hit)",
            "query_protein_name",
            "query_protein_length",
            "length_of_MME(query)",
            "MME(query)_start",
            "MME(query)_end",
            "hit_human_protein_name",
            "hit_human_protein_length",
            "length_of_MME(hit)",
            "MME(hit)_start",
            "MME(hit)_end",
        ]
        for col in core_columns:
            if col not in filtered_df.columns:
                filtered_df[col] = None   # æˆ– np.nan
        other_columns = [
            col for col in filtered_df.columns
            if col not in core_columns
        ]
        filtered_df = filtered_df[core_columns + other_columns]


        print("perfect matchåˆ†æå®Œæˆ")
        IEDB_match_result = IEDB_analyze_matches(filtered_df, IEDB_path, out_dir)
        calc_reference_table(IEDB_match_result, human_location_path, out_dir)
        calc_epitope_table(IEDB_match_result, out_dir)
        calc_query_table(IEDB_match_result, out_dir)
        print("Pipeline å®Œæˆï¼")
        return filtered_df

    except Exception as e:
        error_message = traceback.format_exc()
        print("âŒ Pipeline ç™¼ç”ŸéŒ¯èª¤ï¼š\n", error_message)

    finally:
        # æ¢å¾© stdout/stderr
        sys.stdout = sys.__stdout__
        sys.stderr = sys.__stderr__
        log_file.close()
        